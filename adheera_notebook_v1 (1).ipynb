{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Data Preprocessing"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2024-02-06T08:58:57.797584Z","iopub.status.busy":"2024-02-06T08:58:57.796659Z","iopub.status.idle":"2024-02-06T08:58:57.820548Z","shell.execute_reply":"2024-02-06T08:58:57.819412Z","shell.execute_reply.started":"2024-02-06T08:58:57.797543Z"}},"source":["## 1. Import Libraries"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-02-06T10:14:35.446322Z","iopub.status.busy":"2024-02-06T10:14:35.444862Z","iopub.status.idle":"2024-02-06T10:14:35.458854Z","shell.execute_reply":"2024-02-06T10:14:35.457523Z","shell.execute_reply.started":"2024-02-06T10:14:35.446277Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_13764\\4171968185.py:2: DeprecationWarning: \n","Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n","(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n","but was not found to be installed on your system.\n","If this would cause problems for you,\n","please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n","        \n","  import pandas as pd\n"]},{"ename":"ModuleNotFoundError","evalue":"No module named 'keras.preprocessing.text'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m      4\u001b[0m tqdm\u001b[38;5;241m.\u001b[39mpandas()\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tokenizer\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#from keras.preprocessing.sequence import pad_sequences\u001b[39;00m\n","\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras.preprocessing.text'"]}],"source":["import re\n","import pandas as pd \n","from tqdm import tqdm\n","tqdm.pandas()\n","from keras.preprocessing.text import Tokenizer\n","#from keras.preprocessing.sequence import pad_sequences"]},{"cell_type":"markdown","metadata":{},"source":["## 2. Load and Inspect Data"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-02-06T10:14:40.141604Z","iopub.status.busy":"2024-02-06T10:14:40.141199Z","iopub.status.idle":"2024-02-06T10:14:40.215137Z","shell.execute_reply":"2024-02-06T10:14:40.213920Z","shell.execute_reply.started":"2024-02-06T10:14:40.141570Z"},"trusted":true},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/input/spam-or-not-spam-dataset/spam_or_not_spam.csv'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/input/spam-or-not-spam-dataset/spam_or_not_spam.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mhead())\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1024\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1011\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1012\u001b[0m     dialect,\n\u001b[0;32m   1013\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1020\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1021\u001b[0m )\n\u001b[0;32m   1022\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1024\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:618\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    615\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    617\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 618\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    620\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1618\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1615\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1617\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1618\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1878\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1876\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1877\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1878\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1879\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1880\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1889\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n","\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/spam-or-not-spam-dataset/spam_or_not_spam.csv'"]}],"source":["file_path = \"/kaggle/input/spam-or-not-spam-dataset/spam_or_not_spam.csv\"\n","df = pd.read_csv(file_path)\n","print(df.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-06T10:14:40.922572Z","iopub.status.busy":"2024-02-06T10:14:40.922168Z","iopub.status.idle":"2024-02-06T10:14:40.928483Z","shell.execute_reply":"2024-02-06T10:14:40.927312Z","shell.execute_reply.started":"2024-02-06T10:14:40.922537Z"},"trusted":true},"outputs":[],"source":["print(df.shape)"]},{"cell_type":"markdown","metadata":{},"source":["## 3. Get Class Percentage"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-06T10:14:42.066974Z","iopub.status.busy":"2024-02-06T10:14:42.066560Z","iopub.status.idle":"2024-02-06T10:14:42.075805Z","shell.execute_reply":"2024-02-06T10:14:42.074746Z","shell.execute_reply.started":"2024-02-06T10:14:42.066934Z"},"trusted":true},"outputs":[],"source":["spam_percentage = (df[\"label\"].value_counts()*100/df.shape[0])[1]\n","ham_percentage = (df[\"label\"].value_counts()*100/df.shape[0])[0]\n","print(f\"Percentage of spam emails: {spam_percentage:.2f}%\")"]},{"cell_type":"markdown","metadata":{},"source":["## 4. Function to Remove Given RegEx pattern"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-06T10:14:43.541007Z","iopub.status.busy":"2024-02-06T10:14:43.540637Z","iopub.status.idle":"2024-02-06T10:14:43.547604Z","shell.execute_reply":"2024-02-06T10:14:43.546222Z","shell.execute_reply.started":"2024-02-06T10:14:43.540978Z"},"trusted":true},"outputs":[],"source":["def remove_pattern(text, pattern):\n","    cleaned_text = re.sub(pattern, \"\", str(text))\n","    return \" \".join(cleaned_text.split(\" \"))"]},{"cell_type":"markdown","metadata":{},"source":["## 5. Remove Unnecessary Patterns"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-06T10:15:27.348689Z","iopub.status.busy":"2024-02-06T10:15:27.347392Z","iopub.status.idle":"2024-02-06T10:15:27.381147Z","shell.execute_reply":"2024-02-06T10:15:27.380001Z","shell.execute_reply.started":"2024-02-06T10:15:27.348617Z"},"trusted":true},"outputs":[],"source":["# Lambda expression to remove pattern NUMBER from the text\n","df[\"email\"] = df[\"email\"].progress_apply(lambda x: remove_pattern(x, \"NUMBER\"))\n","\n","# Write Lambda expression to remove pattern URL from the text\n","remove_urls = lambda text: re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n","\n","# Write Lambda expression to remove underscores from the text\n","remove_underscores = lambda text: text.replace(\"_\", \"\")\n","\n","# Write Lambda expression to remove emails from the text\n","remove_emails = lambda text: re.sub(r'\\S+@\\S+', '', text)\n","\n","# Write Lambda expression to remove digits\n","remove_digits = lambda text: re.sub(r'\\d', '', text)"]},{"cell_type":"markdown","metadata":{},"source":["## 6. Function to Replace Text"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-06T10:14:47.022941Z","iopub.status.busy":"2024-02-06T10:14:47.022225Z","iopub.status.idle":"2024-02-06T10:14:47.028851Z","shell.execute_reply":"2024-02-06T10:14:47.027604Z","shell.execute_reply.started":"2024-02-06T10:14:47.022901Z"},"trusted":true},"outputs":[],"source":["def replace_text(source_pattern, destination_pattern, text):\n","    text = re.sub(source_pattern, destination_pattern, str(text))\n","    return text"]},{"cell_type":"markdown","metadata":{},"source":["## 7. Decontraction of Phrases"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-06T10:20:05.971942Z","iopub.status.busy":"2024-02-06T10:20:05.971543Z","iopub.status.idle":"2024-02-06T10:20:05.999446Z","shell.execute_reply":"2024-02-06T10:20:05.998242Z","shell.execute_reply.started":"2024-02-06T10:20:05.971913Z"},"trusted":true},"outputs":[],"source":["# Write Lamda Expression to replace pattern won't to will not\n","df[\"email\"] = df[\"email\"].progress_apply(lambda x: replace_text(x, r\"won't\", \"will not\"))\n","\n","# Write Lamda Expression to replace pattern can't --> can not\n","df[\"email\"] = df[\"email\"].progress_apply(lambda x: replace_text(x, r\"can't\", \"can not\"))\n","\n","# Write Lamda Expression to replace pattern n't --> not\n","df[\"email\"] = df[\"email\"].progress_apply(lambda x: replace_text(x, r\"n't\", \"not\"))\n","\n","# Write Lamda Expression to replace pattern 're --> are\n","df[\"email\"] = df[\"email\"].progress_apply(lambda x: replace_text(x, r\"re\", \"are\"))\n","\n","# Write Lamda Expression to replace pattern 's -> is\n","df[\"email\"] = df[\"email\"].progress_apply(lambda x: replace_text(x, r\"s\", \"is\"))\n","\n","# Write Lamda Expression to replace pattern 'd -> would\n","df[\"email\"] = df[\"email\"].progress_apply(lambda x: replace_text(x, r\"d\", \"would\"))\n","\n","# Write Lamda Expression to replace pattern 'll -> will\n","df[\"email\"] = df[\"email\"].progress_apply(lambda x: replace_text(x, r\"ll\", \"will\"))\n","\n","# Write Lamda Expression to replace pattern 've -> have\n","df[\"email\"] = df[\"email\"].progress_apply(lambda x: replace_text(x, r\"ve\", \"have\"))\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Write function to convert text to lowercase\n","def convert_to_lowercase(text):\n","    return text.lower()\n","\n","# Write function to replace non-alphabets\n","def replace_non_alphabets(text, replacement=' '):\n","    return re.sub(r'[^a-zA-Z]+', replacement, text)"]},{"cell_type":"markdown","metadata":{},"source":["## 8. Convert Feature Column to Categorical"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","# Example DataFrame with a feature column\n","data = {'Feature': ['Category1', 'Category2', 'Category1', 'Category3', 'Category2']}\n","df = pd.DataFrame(data)\n","\n","# Convert the 'Feature' column to categorical\n","df['Feature'] = df['Feature'].astype('category')\n","\n","# Display the DataFrame\n","print(df)\n"]},{"cell_type":"markdown","metadata":{},"source":["## 9. Separate Features and Labels"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","# Example DataFrame with features and labels\n","data = {\n","    'Feature1': [1, 2, 3, 4, 5],\n","    'Feature2': [5, 4, 3, 2, 1],\n","    'Label': ['A', 'B', 'A', 'B', 'A']\n","}\n","\n","df = pd.DataFrame(data)\n","\n","# Separate features and labels\n","features = df.drop('Label', axis=1)  # Drop the 'Label' column\n","labels = df['Label']\n","\n","# Display the separated features and labels\n","print(\"Features:\")\n","print(features)\n","\n","print(\"\\nLabels:\")\n","print(labels)\n"]},{"cell_type":"markdown","metadata":{},"source":["## 10. Perform Train-Test Split"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","# Example DataFrame with features and labels\n","data = {\n","    'Feature1': [1, 2, 3, 4, 5],\n","    'Feature2': [5, 4, 3, 2, 1],\n","    'Label': ['A', 'B', 'A', 'B', 'A']\n","}\n","\n","df = pd.DataFrame(data)\n","\n","# Separate features and labels\n","X = df.drop('Label', axis=1)  # Features\n","y = df['Label']  # Labels\n","\n","# Perform train-test split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Display the shapes of the training and testing sets\n","print(\"X_train shape:\", X_train.shape)\n","print(\"X_test shape:\", X_test.shape)\n","print(\"y_train shape:\", y_train.shape)\n","print(\"y_test shape:\", y_test.shape)\n"]},{"cell_type":"markdown","metadata":{},"source":["## 11. Compute maximum number of words in all emails"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","# Assuming you have a DataFrame with an 'Emails' column\n","# Replace 'your_data.csv' with the actual file containing your email data\n","df = pd.read_csv('/kaggle/input/spam-or-not-spam-dataset/spam_or_not_spam.csv')\n","\n","# Assuming the 'Emails' column contains the text of emails\n","# You may need to preprocess the text if required (e.g., remove punctuation, lowercasing, etc.)\n","\n","# Split each email into words and calculate the number of words in each email\n","df['Word_Count'] = df['Emails'].apply(lambda x: len(str(x).split()))\n","\n","# Find the maximum number of words across all emails\n","max_words = df['Word_Count'].max()\n","\n","print(\"Maximum number of words in all emails:\", max_words)\n"]},{"cell_type":"markdown","metadata":{},"source":["## 12. Create Keras preprocessing Tokenizer Object"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Fit Tokenizer object on train data\n","# Encode X_train and X_test using the Tokenizer object\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","# Assuming you have a DataFrame with a 'Text' column\n","# Replace 'your_data.csv' with the actual file containing your text data\n","df = pd.read_csv('/kaggle/input/spam-or-not-spam-dataset/spam_or_not_spam.csv')\n","\n","# Assuming 'Text' column contains the text data\n","X = df['Text'].values\n","y = df['Label'].values  # Assuming you have labels, adjust accordingly\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Create a Tokenizer object\n","tokenizer = Tokenizer()\n","\n","# Fit the tokenizer on the training data\n","tokenizer.fit_on_texts(X_train)\n","\n","# Encode the training and testing data using the fitted tokenizer\n","X_train_encoded = tokenizer.texts_to_sequences(X_train)\n","X_test_encoded = tokenizer.texts_to_sequences(X_test)\n","\n","# Example of the encoded sequences\n","print(\"Encoded X_train example:\")\n","print(X_train_encoded[0])\n","\n","# If you want to pad the sequences to have the same length\n","max_sequence_length = max(len(seq) for seq in X_train_encoded + X_test_encoded)\n","X_train_padded = pad_sequences(X_train_encoded, maxlen=max_sequence_length, padding='post')\n","X_test_padded = pad_sequences(X_test_encoded, maxlen=max_sequence_length, padding='post')\n","\n","# Example of the padded sequences\n","print(\"\\nPadded X_train example:\")\n","print(X_train_padded[0])\n"]},{"cell_type":"markdown","metadata":{},"source":["## 13. Pad Input Sequences using Keras preprocessing pad_sequences function"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Pad the sequences to have the same length\n","max_sequence_length = 100  # Adjust this based on your specific requirements\n","X_train_padded = pad_sequences(X_train_encoded, maxlen=max_sequence_length, padding='post')\n","X_test_padded = pad_sequences(X_test_encoded, maxlen=max_sequence_length, padding='post')\n","\n","# Display the shapes of the padded sequences\n","print(\"X_train_padded shape:\", X_train_padded.shape)\n","print(\"X_test_padded shape:\", X_test_padded.shape)"]},{"cell_type":"markdown","metadata":{},"source":["## 14. Create Character Embedding "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Create a Sequential model\n","model = Sequential()\n","\n","# Add an Embedding layer for character embeddings\n","embedding_dim = 50  # Adjust this based on your specific requirements\n","model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=embedding_dim, input_length=max_sequence_length))\n","\n","# Add an LSTM layer (you can use GRU or other recurrent layers as well)\n","model.add(LSTM(100))\n","\n","# Add a Dense layer for classification (adjust units based on your specific task)\n","model.add(Dense(1, activation='sigmoid'))\n","\n","# Compile the model\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# Display the model summary\n","print(model.summary())"]},{"cell_type":"markdown","metadata":{},"source":["# Modeling"]},{"cell_type":"markdown","metadata":{},"source":["## 15. Define Model Architecture Using Functional API"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define the model using the Functional API\n","inputs = Input(shape=(max_sequence_length,))\n","embedding_dim = 50  # Adjust this based on your specific requirements\n","\n","embedding_layer = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=embedding_dim, input_length=max_sequence_length)(inputs)\n","bidirectional_lstm = Bidirectional(LSTM(100))(embedding_layer)\n","outputs = Dense(1, activation='sigmoid')(bidirectional_lstm)\n","\n","model = Model(inputs=inputs, outputs=outputs)\n","\n","# Compile the model\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# Display the model summary\n","print(model.summary())"]},{"cell_type":"markdown","metadata":{},"source":["## 16. Define Optimizer (e.g. Adam)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from tensorflow.keras.optimizers import Adam\n","\n","# ... (previous code for data preprocessing and model definition)\n","\n","# Compile the model with the Adam optimizer\n","optimizer = Adam(learning_rate=0.001)  # You can adjust the learning rate based on your needs\n","\n","model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n"]},{"cell_type":"markdown","metadata":{},"source":["## 17. Write class for custom callback for micro-f1 score"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["## 18. Create Modelcheckpoint Callback"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["## 19. Create Callback for TensorBoard"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["## 20. Compile the Model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["## 21. Train the model using model.fit()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["## 22. Plot Performance Curves Using history object"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["## 23. Plot Model Architecture Using keras.utils plot_model function"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":91827,"sourceId":213216,"sourceType":"datasetVersion"}],"dockerImageVersionId":30646,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.2"}},"nbformat":4,"nbformat_minor":4}
